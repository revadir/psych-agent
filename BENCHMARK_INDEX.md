# ğŸ“‹ LLM Benchmark Comparison Tool - Index

## ğŸ¯ Overview

A complete system for benchmarking and comparing psychiatric knowledge across multiple LLMs (Llama 3 and Mistral).

---

## ğŸ“ New Files Created

### Main Benchmarking Tool
```
scripts/
â”œâ”€â”€ compare_llm_benchmarks.py      â­ MAIN TOOL
â”‚   â€¢ 500+ line comprehensive benchmarking engine
â”‚   â€¢ Loads original questions and Llama 3 results
â”‚   â€¢ Queries Mistral LLM via Ollama
â”‚   â€¢ Calculates similarity and concept analysis
â”‚   â€¢ Generates detailed reports
â”‚   â€¢ Exports JSON results
```

### Execution Scripts
```
â”œâ”€â”€ run_benchmark.bat              ğŸ’» WINDOWS RUNNER
â”‚   â€¢ Double-click to run on Windows
â”‚   â€¢ Handles all setup automatically
â”‚   
â”œâ”€â”€ run_benchmark.sh               ğŸ§ UNIX RUNNER  
â”‚   â€¢ For Mac/Linux systems
â”‚   â€¢ chmod +x && ./run_benchmark.sh
```

### Utility Tools
```
â”œâ”€â”€ demo_benchmark.py              ğŸ“ INTERACTIVE DEMO
â”‚   â€¢ Learn how to use the tool
â”‚   â€¢ Test individual features
â”‚   â€¢ Step-by-step examples
â”‚   â€¢ Interactive menu
â”‚
â”œâ”€â”€ setup_ollama.py                ğŸ”§ SETUP VERIFICATION
â”‚   â€¢ Checks Ollama installation
â”‚   â€¢ Verifies Mistral model
â”‚   â€¢ Tests connectivity
â”‚   â€¢ Recommends fixes
```

### Documentation
```
â”œâ”€â”€ QUICK_START.md                 âš¡ FAST GUIDE (5 min)
â”‚   â€¢ Prerequisites
â”‚   â€¢ How to run
â”‚   â€¢ Understanding output
â”‚   â€¢ Quick troubleshooting
â”‚
â”œâ”€â”€ BENCHMARK_GUIDE.md             ğŸ“– COMPLETE REFERENCE
â”‚   â€¢ Detailed features
â”‚   â€¢ Installation steps
â”‚   â€¢ Advanced usage
â”‚   â€¢ Performance optimization
â”‚   â€¢ Citation info
â”‚
â”œâ”€â”€ BENCHMARK_SUMMARY.md           ğŸ“Š EXECUTIVE SUMMARY
â”‚   â€¢ What was created
â”‚   â€¢ Feature list
â”‚   â€¢ Usage examples
â”‚   â€¢ Analysis workflow
â”‚
â”œâ”€â”€ README_BENCHMARKS.md           ğŸ“š DIRECTORY GUIDE
â”‚   â€¢ File overview
â”‚   â€¢ Tool descriptions
â”‚   â€¢ Usage patterns
â”‚   â€¢ Troubleshooting
```

### Main Documentation
```
root/
â”œâ”€â”€ INSTALLATION_COMPLETE.md       âœ… COMPLETION SUMMARY
    â€¢ What was created
    â€¢ Quick start guide
    â€¢ Feature overview
    â€¢ Next steps
```

---

## ğŸš€ Quick Start

### Step 1: Prerequisites (One-time setup)
```bash
# Install Ollama from https://ollama.ai
ollama pull mistral
```

### Step 2: Start Ollama Server
```bash
ollama serve
```
Keep this running in background!

### Step 3: Run Benchmark
```bash
cd c:\Users\sushm\psych-agent\scripts

# Windows:
run_benchmark.bat

# Mac/Linux:
python compare_llm_benchmarks.py
```

### Step 4: Review Results
- Read console output for summary
- Check JSON file in backend/ for detailed results

---

## ğŸ“Š What Each Tool Does

### compare_llm_benchmarks.py (MAIN)
**The Core Benchmarking Engine**

Features:
- Load 60 psychiatric questions with expected answers
- Load Llama 3 pre-computed responses
- Query Mistral LLM with same questions
- Calculate similarity (0-1 scale)
- Extract and analyze concepts
- Compare all three sources
- Generate detailed reports
- Export JSON results

Usage:
```bash
python compare_llm_benchmarks.py
```

### demo_benchmark.py (LEARNING)
**Interactive Learning Tool**

Run demos for:
1. Loading benchmark data
2. Querying Mistral
3. Calculating similarity
4. Concept extraction
5. Full evaluation workflow

Usage:
```bash
python demo_benchmark.py
# Choose demos from interactive menu
```

### setup_ollama.py (VERIFICATION)
**Setup Verification & Troubleshooting**

Checks:
- âœ“ Ollama installed
- âœ“ Ollama server running
- âœ“ Models available
- âœ“ Mistral model installed
- âœ“ Can query Mistral

Usage:
```bash
python setup_ollama.py
```

### run_benchmark.bat / .sh (EXECUTION)
**One-Click Benchmark Running**

Windows:
```cmd
run_benchmark.bat
```

Unix/Linux:
```bash
./run_benchmark.sh
```

---

## ğŸ“ˆ Benchmark Details

### Questions Covered
- **60 Total Questions** across 6 sections
- **Sections**:
  1. Psychiatric Knowledge (Q1-10)
  2. Clinical Understanding (Q11-20)
  3. ICD-10 Diagnosis (Q21-30)
  4. Differential Diagnosis (Q31-40)
  5. Medications (Q41-50)
  6. Long-term Management (Q51-60)

### Sources Compared
1. **Original Answers** - Reference/source answers
2. **Llama 3** - Pre-computed results
3. **Mistral** - Real-time queries via Ollama

### Metrics Calculated
- **Similarity Score** (0-1 scale)
- **Concept Overlap** (% of key terms)
- **Status Rating** (EXCELLENT/GOOD/PARTIAL/POOR)
- **Section Performance** (per section)
- **Overall Assessment** (final score)

---

## ğŸ“– Documentation Guide

| Document | Read Time | Purpose | Audience |
|----------|-----------|---------|----------|
| **QUICK_START.md** | 5 min | Get running | Everyone |
| **demo_benchmark.py** | 10 min | Learn tool | Developers |
| **BENCHMARK_GUIDE.md** | 15 min | Deep dive | Analysts |
| **Code comments** | 20 min | Advanced | Developers |

### Reading Path

**For Quick Setup (15 min):**
1. Read INSTALLATION_COMPLETE.md
2. Read QUICK_START.md
3. Run setup_ollama.py
4. Run compare_llm_benchmarks.py

**For Learning (45 min):**
1. Read QUICK_START.md
2. Run demo_benchmark.py
3. Read BENCHMARK_GUIDE.md
4. Run benchmark and analyze

**For Deep Understanding (2 hours):**
1. Read all docs
2. Run all demos
3. Review source code
4. Create custom analysis

---

## âš¡ Performance Guide

| Task | Time | Notes |
|------|------|-------|
| Load data | ~2 sec | File I/O |
| Query Mistral | 5-15 min | 60 questions |
| Analysis | ~30 sec | Calculations |
| Report generation | ~10 sec | Formatting |
| **Total** | **6-20 min** | Hardware dependent |

Factors affecting speed:
- First run slower (model cache)
- GPU faster than CPU
- Ollama memory usage
- Network latency

---

## ğŸ› ï¸ Troubleshooting Flow

```
Issue: Cannot connect to Ollama
â””â”€> Fix: ollama serve

Issue: Mistral not found
â””â”€> Fix: ollama pull mistral

Issue: Very slow
â””â”€> Fix: Wait (first run) or check GPU

Issue: Questions not found
â””â”€> Fix: Verify backend/ files exist

Issue: Still stuck?
â””â”€> Run: python setup_ollama.py
```

---

## ğŸ’¡ Usage Examples

### Example 1: One-Click (Easiest)
```cmd
run_benchmark.bat
```
Done! Check results.

### Example 2: Python Direct
```bash
python compare_llm_benchmarks.py
```

### Example 3: Interactive Learning
```bash
python demo_benchmark.py
```
Choose demos to explore.

### Example 4: Setup Verification
```bash
python setup_ollama.py
```
Verify everything working.

### Example 5: Python API
```python
from compare_llm_benchmarks import BenchmarkComparison

benchmark = BenchmarkComparison()
benchmark.run_full_benchmark()
```

---

## ğŸ“Š Expected Output

### Console Report Example
```
================================================================================
PSYCHBENCH LLM COMPARISON REPORT
================================================================================

ğŸ“‹ Section: Psychiatric Knowledge
Mistral Average Similarity: 0.78
Status Distribution (Mistral):
  EXCELLENT: 7 (70.0%)
  GOOD: 2 (20.0%)
  PARTIAL: 1 (10.0%)

Mistral Performance Assessment: GOOD
Overall Similarity Score: 0.72
```

### Files Generated
```
backend/
â””â”€â”€ PsychBench-benchmark-mistral-results-2026-01-10T12-34-56.json
```

---

## âœ… System Requirements

**Required:**
- Python 3.8+
- Ollama (https://ollama.ai)
- 8 GB RAM minimum
- 20 GB disk (for Mistral)
- Internet (initial download)

**Recommended:**
- Python 3.10+
- 16 GB RAM
- 50 GB disk
- NVIDIA GPU with CUDA

---

## ğŸ¯ Next Steps

### Immediate (Now)
1. [ ] Verify Ollama installed
2. [ ] Run `ollama pull mistral`
3. [ ] Start `ollama serve`

### Short-term (Today)
1. [ ] Run `python setup_ollama.py`
2. [ ] Run `run_benchmark.bat` or Python version
3. [ ] Review results

### Medium-term (This Week)
1. [ ] Run `python demo_benchmark.py`
2. [ ] Read BENCHMARK_GUIDE.md
3. [ ] Analyze results by section
4. [ ] Identify improvement areas

### Long-term (Ongoing)
1. [ ] Run benchmarks periodically
2. [ ] Track performance trends
3. [ ] Test new models
4. [ ] Optimize prompts

---

## ğŸ” File Locations

### Scripts Directory
```
scripts/
â”œâ”€â”€ compare_llm_benchmarks.py      [MAIN TOOL]
â”œâ”€â”€ demo_benchmark.py              [LEARNING]
â”œâ”€â”€ setup_ollama.py                [VERIFICATION]
â”œâ”€â”€ run_benchmark.bat              [WINDOWS]
â”œâ”€â”€ run_benchmark.sh               [UNIX]
â”œâ”€â”€ QUICK_START.md                 [5-MIN GUIDE]
â”œâ”€â”€ BENCHMARK_GUIDE.md             [FULL DOCS]
â”œâ”€â”€ BENCHMARK_SUMMARY.md           [SUMMARY]
â””â”€â”€ README_BENCHMARKS.md           [DIRECTORY]
```

### Root Directory
```
root/
â””â”€â”€ INSTALLATION_COMPLETE.md       [THIS INDEX]
```

### Backend Directory (Generated)
```
backend/
â””â”€â”€ PsychBench-benchmark-mistral-results-*.json
```

---

## ğŸ“ Getting Help

1. **Setup Issues** â†’ Run `python setup_ollama.py`
2. **How to Use** â†’ Read `QUICK_START.md`
3. **Learn Features** â†’ Run `python demo_benchmark.py`
4. **Deep Dive** â†’ Read `BENCHMARK_GUIDE.md`
5. **Troubleshooting** â†’ Check QUICK_START.md section

---

## âœ¨ Key Features Summary

- âœ… Automated multi-LLM comparison
- âœ… Semantic similarity analysis
- âœ… Concept extraction & overlap
- âœ… Comprehensive reporting
- âœ… JSON export for analysis
- âœ… Easy-to-use interface
- âœ… Cross-platform (Windows/Mac/Linux)
- âœ… Extensive documentation
- âœ… Interactive learning tools
- âœ… Setup verification

---

## ğŸ“ Version Info

- **Version**: 1.0
- **Created**: January 10, 2026
- **Status**: Production Ready âœ…
- **Python Required**: 3.8+
- **Ollama Required**: Latest version

---

## ğŸš€ You're Ready!

Everything is installed and documented. 

**Start here:**
```bash
# Step 1: Verify setup
python scripts/setup_ollama.py

# Step 2: Run benchmark  
cd scripts
run_benchmark.bat  # Windows
# or
python compare_llm_benchmarks.py  # Any OS
```

**Questions?**
- Quick questions â†’ QUICK_START.md
- How-to questions â†’ demo_benchmark.py
- Technical questions â†’ BENCHMARK_GUIDE.md

**Time needed:** 15 minutes to first results!

---

**Happy benchmarking! ğŸ§ **
